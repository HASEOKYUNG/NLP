{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebc9da9",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77bfd6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "import warnings;warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ed01d",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6473b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "satur_raw = pd.read_csv('../data/satur/satur_20240313.csv', usecols=[1], names=['pro_nm'], encoding='utf-8')\\\n",
    "            .drop_duplicates().reset_index(drop=True).reset_index().rename(columns={'index':'data_id'})\n",
    "satur_product = pd.read_csv('../data/satur/satur_product.csv', usecols=['enc_nm','kor_nm'], encoding='cp949')\\\n",
    "            .drop_duplicates().reset_index(drop=True).reset_index().rename(columns={'index':'product_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc5664",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b8d1445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 불필요한 특수문자를 제거한다.\n",
    "# [참고] MassAdoption\\3월\\240313_SaturAssociationRule\\1. DataCleansing\\3. Cleansing_ProductTable.ipynb\n",
    "satur_raw['DC_pro_nm'] = satur_raw.pro_nm.apply(lambda x: x[x.rindex('>')+1:] if '<' in x else x)\\\n",
    "                        .apply(lambda x: x[x.rindex(')')+1:] if '(' in x else x)\\\n",
    "                        .apply(lambda x: x[x.rindex(']')+1:] if '[' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da53ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "satur_product['DC_enc_nm'] = satur_product['enc_nm']\\\n",
    "                             .apply(lambda x: x[x.rindex('>')+1:] if '<' in x else x)\\\n",
    "                             .apply(lambda x: x[x.rindex(')')+1:] if '(' in x else x)\\\n",
    "                             .apply(lambda x: x[x.rindex(']')+1:] if '[' in x else x).str.replace('\\n','').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca6f50f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "satur_product['DC_kor_nm'] = satur_product['kor_nm'].fillna('-')\\\n",
    "                             .apply(lambda x: x[x.rindex('>')+1:] if '<' in x else x)\\\n",
    "                             .apply(lambda x: x[x.rindex(')')+1:] if '(' in x else x)\\\n",
    "                             .apply(lambda x: x[x.rindex(']')+1:] if '[' in x else x).str.replace('\\n','').str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf41365",
   "metadata": {},
   "source": [
    "## Split color info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a5323ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "getColor = satur_product[satur_product.enc_nm.str.contains('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1890cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "getColor['Color_enc_nm'] = getColor.DC_enc_nm.str[::-1].str.split(' - ', expand=True)[0].str[::-1]\n",
    "getColor['Product_enc_nm'] = getColor[['DC_enc_nm','Color_enc_nm']]\\\n",
    "                             .apply(lambda x: x[0][:x[0].index(x[1])-2] if x[1] in x[0] else x[0], axis=1)\n",
    "# 영어 상품명\n",
    "TRAIN_ENC_DATA = [(i, {\"entities\":[(i.rindex('-')+2,len(i), \"COLOR\")]}) for i in getColor.DC_enc_nm.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a600297",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_colors = getColor.Color_enc_nm.str.count(' ')+1\n",
    "getColor['Color_kor_nm'] = [' '.join(COLOR.split()[-N:]) for COLOR, N in list(zip(getColor.DC_kor_nm, n_colors)) if N]\n",
    "getColor['Product_kor_nm'] = getColor[['DC_kor_nm','Color_kor_nm']]\\\n",
    "                             .apply(lambda x: x[0][:x[0].index(x[1])-1] if x[1] in x[0] else x[0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58b32e",
   "metadata": {},
   "source": [
    "## spaCy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6632c536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 405.9557896050791}\n",
      "{'ner': 93.20782349693889}\n",
      "{'ner': 40.44284183418084}\n",
      "{'ner': 42.33473774070164}\n",
      "{'ner': 20.663546943535202}\n",
      "{'ner': 18.85016809055798}\n"
     ]
    }
   ],
   "source": [
    "# 영어 모델 학습\n",
    "enc_nlp = spacy.blank(\"en\")\n",
    "if \"ner\" not in enc_nlp.pipe_names:\n",
    "    ner = enc_nlp.create_pipe(\"ner\")\n",
    "    enc_nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = enc_nlp.get_pipe(\"ner\")\n",
    "\n",
    "# 'COLOR' 엔티티 추가\n",
    "ner.add_label(\"COLOR\")\n",
    "\n",
    "# 모델 훈련\n",
    "optimizer = enc_nlp.begin_training()\n",
    "for itn in range(6):  # 6번의 반복 훈련\n",
    "    random.shuffle(TRAIN_ENC_DATA)\n",
    "    losses = {}\n",
    "    for text, annotations in TRAIN_ENC_DATA:\n",
    "        doc = enc_nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        enc_nlp.update([example], drop=0.5, losses=losses)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ca6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# 한글 모델 학습\n",
    "# mecab-ko, mecab-ko-dic, natto 설치가 필요하다. 작동하지 않을 경우 enc_nm color의 글자로 유추해 변환한다.\n",
    "kor_nlp = spacy.blank(\"ko\")\n",
    "if \"ner\" not in kor_nlp.pipe_names:\n",
    "    ner = kor_nlp.create_pipe(\"ner\")\n",
    "    kor_nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = kor_nlp.get_pipe(\"ner\")\n",
    "\n",
    "# 'COLOR' 엔티티 추가\n",
    "ner.add_label(\"COLOR\")\n",
    "\n",
    "# 모델 훈련\n",
    "optimizer = kor_nlp.begin_training()\n",
    "for itn in range(6):  # 6번의 반복 훈련\n",
    "    random.shuffle(TRAIN_KOR_DATA)\n",
    "    losses = {}\n",
    "    for text, annotations in TRAIN_KOR_DATA:\n",
    "        doc = kor_nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        kor_nlp.update([example], drop=0.5, losses=losses)\n",
    "    print(losses)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa86edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "class KoreanTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = Okt()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = self.tokenizer.morphs(text)\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# 빈 모델 생성 및 한국어 토크나이저 설정\n",
    "nlp = spacy.blank(\"ko\")\n",
    "nlp.tokenizer = KoreanTokenizer(nlp.vocab)\n",
    "\n",
    "# NER 파이프라인 추가 (필요한 경우)\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(\"ner\", last=True)\n",
    "\n",
    "# 모델 사용 예시\n",
    "doc = nlp(\"한국어 문장을 여기에 입력하세요.\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# 다국어 모델 로드\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "\n",
    "# 모델 사용 예시\n",
    "doc = nlp(\"이 문장에는 서울, 뉴욕, 파리 같은 지명이 포함되어 있습니다.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "879d9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 색상 규칙에 부합하지 않는 데이터에 자연어 처리를 한다.\n",
    "# enc_nlp = pickle.load(open('../data/satur/color_ner_model.pkl','rb'))\n",
    "unrule = satur_product.loc[[i for i in satur_product.index if i not in getColor.index]]\n",
    "\n",
    "unrule['Color_enc_nm'] = [enc_nlp(i).ents for i in unrule.DC_enc_nm]\n",
    "# unrule['Color_enc_nm'].apply(lambda x: x[0].text if len(x)!= 0 else '-')\n",
    "unrule['Color_enc_nm'] = ['French Brown','-','-','-','Black','Ivory',\n",
    "                          '-','Melange Ivory','-','-','Summer Forest',\n",
    "                          'Navy','Ivory','Saddle Ivory','-','-','Ivory',\n",
    "                          'Oyster Gray','-','-','-','-','-','-']\n",
    "# 오류 데이터 제거\n",
    "unrule.drop([344], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb11cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 색상명 병합, 영어 상품명 생성\n",
    "unrule['Product_enc_nm'] = unrule[['DC_enc_nm','Color_enc_nm']]\\\n",
    "                           .apply(lambda x: x[0] if x[1] not in x[0] else x[0][:x[0].index(x[1])-1], axis=1)\n",
    "unrule.loc[[648,686], 'Product_enc_nm'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c43c9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 색상명 병합(글자 수), 한글 상품명 생성\n",
    "unrule['Color_kor_nm'] = ['프렌치 브라운','-','-','-','블랙','아이보리',\n",
    "                          '-','-','-','썸머 포레스',\n",
    "                          '네이비','아이보리','새들 아이보리','-','-','아이보리',\n",
    "                          '오이스터 그레이','-','-','-','-','-','-']\n",
    "unrule['Product_kor_nm'] = unrule[['DC_kor_nm','Color_kor_nm']]\\\n",
    "                           .apply(lambda x: x[0] if x[1] not in x[0] else x[0][:x[0].index(x[1])-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d97ded80",
   "metadata": {},
   "outputs": [],
   "source": [
    "DC_satur_product = pd.concat([getColor, unrule])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a60e13",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04f4543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLOR NER 모델을 저장한다.\n",
    "# pickle.dump((enc_nlp), open('../data/satur/color_ner_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d48ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DC_satur_product.to_csv('../data/satur/DC_satur_product.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa0c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
